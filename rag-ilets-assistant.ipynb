{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf93e36a",
   "metadata": {},
   "source": [
    "### This notebook demonstrates a full RAG pipeline:\n",
    "\n",
    "- Downloading and transcribing YouTube audio\n",
    "\n",
    "- Chunking and embedding the transcript\n",
    "\n",
    "- Building a semantic search index\n",
    "\n",
    "- Enabling strict, reference-based conversational QA and summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ae069",
   "metadata": {},
   "source": [
    "## 1. Import RAG Pipeline and Agent Utilities\n",
    "\n",
    "In this cell, we import all the necessary functions and classes from our custom modules (`rag_pipeline.py` and `agent.py`).  \n",
    "These utilities handle environment setup, audio downloading, transcription, document splitting, embedding, vector search, and agent construction for conversational QA and summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RAG pipeline utilities\n",
    "from rag_pipeline import (\n",
    "\n",
    "    load_environment,\n",
    "    download_audio,\n",
    "    process_audio,\n",
    "    load_and_split_documents,\n",
    "    create_vectorstore,\n",
    "    build_qa_chain\n",
    ")\n",
    "\n",
    "# Import agent utilities\n",
    "from agent import build_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900fb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80fd15b",
   "metadata": {},
   "source": [
    "## 2. Download Audio from YouTube\n",
    "\n",
    "Here, we specify the YouTube URL for the IELTS video we want to process.  \n",
    "The `download_audio` function downloads the audio track of the video and saves it as an MP3 file in the output directory.  \n",
    "The resulting path is displayed for confirmation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375bb1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url = \"https://www.youtube.com/watch?v=kHTnAx6f-j0&list=PLWWR_9t3vo3OfJ62HL-nnwRaSAiLnaMSM&index=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ec38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = download_audio(youtube_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e254b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887eda00",
   "metadata": {},
   "source": [
    "## 3. Load Environment and Transcribe Audio\n",
    "\n",
    "This cell loads your OpenAI API key and initializes the LLM and OpenAI client.  \n",
    "Then, it processes the downloaded audio:  \n",
    "- Splits it into manageable chunks  \n",
    "- Transcribes each chunk using OpenAI Whisper  \n",
    "- Saves the transcript to a text file  \n",
    "This is the core step for converting video content into searchable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key, llm, clinet =load_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_audio(client=clinet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a0368",
   "metadata": {},
   "source": [
    "## 4. Load and Split Transcript Documents\n",
    "\n",
    "After transcription, we need to load the transcript files and split them into smaller text chunks.  \n",
    "This helps with efficient embedding and retrieval, as semantic search works best on small, focused pieces of text.  \n",
    "The resulting list of document chunks is displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_and_split_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310a504",
   "metadata": {},
   "source": [
    "## 5. Assign Source Metadata to Document Chunks\n",
    "\n",
    "For traceability, each chunk is annotated with metadata indicating its source (the original video or audio file).  \n",
    "This is useful for providing references when answering questions and for debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for chunk in docs:\n",
    "    video_id = os.path.splitext(os.path.basename(chunk.metadata[\"source\"]))[0]\n",
    "    chunk.metadata[\"source\"] = video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14305ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b3ad9",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings and Build FAISS Vector Store\n",
    "\n",
    "Now, we generate vector embeddings for each document chunk using OpenAI embeddings.  \n",
    "These embeddings are stored in a FAISS vector index, enabling fast semantic search and retrieval for question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = create_vectorstore(api_key=api_key, docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf67db",
   "metadata": {},
   "source": [
    "## 7. Create a Strict QA Prompt and Build QA Chain (Retriever)\n",
    "\n",
    "We define a strict prompt template to ensure the model only answers questions based on the transcript, not external knowledge.  \n",
    "Then, we build a RetrievalQA chain that uses the vector store and the prompt to answer questions, returning both the answer and the supporting source documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a604248",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = build_qa_chain(vector_store=vectorstore, chat_model=llm, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf413b31",
   "metadata": {},
   "source": [
    "## 8. Initialize and Test the Conversational Agent\n",
    "\n",
    "Finally, we build a conversational agent that can answer questions and provide summaries, strictly using the transcript content.  \n",
    "We test the agent with a series of queries to check that it responds appropriately, refusing to answer questions not covered in the transcript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcad725",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = build_agent(chat_model=llm , chain= qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d41846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "with tracing_v2_enabled():\n",
    "    print(agent.invoke(\"What is the key difference between how band 5–6 students and band 7–9 students answer in Part 1 of the test? Provide one example that illustrates this difference.\"))  # Should store name\n",
    "    print(agent.invoke(\"What does the teacher mean by “test mode,” and why does adopting that mode negatively affect students’ scores?\"))  # Should return \"Your name is Layla\"\n",
    "    print(agent.invoke(\"Which strategy should candidates avoid when addressing each bullet point on the cue card, and what do band 9 candidates do instead?\"))  \n",
    "    print(agent.invoke(\"If I face a Part 3 question on a topic I know little about, what steps are recommended to still give an acceptable answer?\")) \n",
    "    print(agent.invoke(\"When answering a question like “What skills does a person need to be a great chef?”, what four stages do band 9 students go through in building their response?\"))\n",
    "    print(agent.invoke(\"Summarize the content of video transcript \"))\n",
    "    print(agent.invoke(\"what was the first question I asked\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
