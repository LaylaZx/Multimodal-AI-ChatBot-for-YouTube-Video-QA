{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a25591",
   "metadata": {},
   "source": [
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce3955",
   "metadata": {},
   "source": [
    "The project requires several packages that need to be installed into Workspace:\n",
    "\n",
    "- Langchain: is a framework for developing generative Al applications.\n",
    "- yt_dip: lets you download YouTube vide\n",
    "- tiktoken: converts text into tokens.\n",
    "- docarray: makes it easier to work with multi-model data (in this case mixing audio and text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4b2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ac0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15328812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d97434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d62ea7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba155bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers soundfile langchain docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3644f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp as youtube_dl\n",
    "from yt_dlp import DownloadError\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9b931",
   "metadata": {},
   "source": [
    "### **YouTube Audio Extraction & MP3 Conversion**\n",
    "This snippet specifies an output folder and a target YouTube URL, then configures yt_dlp to:\n",
    "\n",
    "1. Download the highest-quality audio stream available.\n",
    "\n",
    "2. Use FFmpeg to extract and convert that stream into a 192 kbps MP3.\n",
    "\n",
    "3. Name the resulting file after the video’s title.\n",
    "\n",
    "4. Enable verbose logging so you can see detailed progress.\n",
    "\n",
    "It wraps the download call in a try/except block to catch and report any DownloadError that might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7013578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output/\"\n",
    "youtube_url =\"https://youtu.be/4h9lQfYLOZU?si=4Z4RCfJjaAdAp2e1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8207d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for yt_dlp\n",
    "def download_audio(url: str) -> str:\n",
    "    \n",
    "    ydl_config = {\n",
    "        \"format\": \"bestaudio/best\",\n",
    "        # List of post-processing steps; each dict represents one processor\n",
    "        \"postprocessors\": [\n",
    "            {\n",
    "                \"key\": \"FFmpegExtractAudio\",   # Extract audio from the downloaded file\n",
    "                \"preferredcodec\": \"mp3\",       # Convert audio to MP3 format\n",
    "                \"preferredquality\": \"192\"      # Set audio quality to 192 kbps\n",
    "            }\n",
    "        ],\n",
    "        # Template for naming the output file: \"<video title>.<extension>\"\n",
    "        \"outtmpl\": \"output/%(title)s.%(ext)s\",\n",
    "        \"verbose\": True  # Enable detailed logging\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Initialize the downloader with the specified config and start download\n",
    "        with youtube_dl.YoutubeDL(ydl_config) as ydl:\n",
    "            info = ydl.extract_info(url, download=True)\n",
    "    except DownloadError as e:\n",
    "        # Print any download errors that occur\n",
    "        print(\"DownloadError:\", e)\n",
    "    \n",
    "    return f\"Downloaded: output_audio/{info['id']}.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53798b39",
   "metadata": {},
   "source": [
    "### **Batch Transcription of MP3 Files with Whisper**\n",
    "\n",
    "1. Detects whether to run on GPU (CUDA) or CPU and sets the optimal PyTorch data type for memory efficiency.\n",
    "\n",
    "2. Loads the openai/whisper-large-v3 model (using safetensors for faster, lower-memory loading) and moves it to the chosen device.\n",
    "\n",
    "3. Loads the accompanying processor (tokenizer + feature extractor).\n",
    "\n",
    "4. Builds an ASR pipeline around that model + processor.\n",
    "\n",
    "5. Uses glob to collect all .mp3 files in the specified output_dir and validates that at least one file exists.\n",
    "\n",
    "6. Defines a single transcript output path (files/transcripts/transcript.txt) and ensures its folder is created.\n",
    "\n",
    "7. Iterates over each MP3, reads the audio into an array, runs the ASR pipeline to generate text, and writes the resulting transcript (overwriting on each loop) to the designated text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21fe9737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Figure out if I can use GPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#    and choose torch dtype to save memory on GPU\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# 2. Pick the Whisper model I want\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "# 3. Load the model weights (using safetensors to speed things up)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "# move the model to the right device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0105aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load the processor that has both tokenizer & feature extractor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf903bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# 5. Build the ASR pipeline with my model and processor\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Find all the MP3 files in my output directory\n",
    "audio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\n",
    "\n",
    "# 7. Make sure I actually have files to process\n",
    "if not audio_files:\n",
    "    raise ValueError(f\"No .mp3 files found in {output_dir}\")\n",
    "\n",
    "# 8. Loop through each file, transcribe, and save\n",
    "# Where to save the text transcript\n",
    "output_file  = \"files/transcripts/transcript.txt\"\n",
    "\n",
    "for audio_path in audio_files:\n",
    "    print(f\"Processing {audio_path}...\")\n",
    "    #    read the audio array and sampling rate\n",
    "    audio_array, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "    sample = {\"array\": audio_array, \"sampling_rate\": sr}\n",
    "\n",
    "    #    run the pipeline to get the transcript\n",
    "    result = pipe(sample, return_timestamps=True)\n",
    "    text = result[\"text\"]\n",
    "\n",
    "    #    prepare output path & ensure folder exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    #    write the transcript to disk\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"Saved transcript to {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from dotenv import load_dotenv\n",
    "# Load .env into environment\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Initialize the new OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# —————— Transcription ——————\n",
    "print(\"Converting audio to text…\")\n",
    "\n",
    "\n",
    "with open(audio_filename, \"rb\") as f:\n",
    "    # Call the new transcription endpoint\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file=f,            # the binary audio file\n",
    "        model=\"whisper-1\"  # the current Whisper model name\n",
    "    )\n",
    "\n",
    "# Extract the plain text\n",
    "text = transcription.text\n",
    "\n",
    "# —————— Save to disk ——————\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(text)\n",
    "\n",
    "print(f\"Transcript saved to {output_file}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc0b14",
   "metadata": {},
   "source": [
    "### **Load Transcript File into LangChain Documents** \n",
    "\n",
    "Uses LangChain’s TextLoader to read transcript.txt and convert it into a list of Document objects for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d1c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the TextLoader class, specifying the directory containing the text files\n",
    "loader = TextLoader(\"./files/transcripts/transcript.txt\")\n",
    "\n",
    "# Load the documents from the specified directory using the TextLoader instance\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e69be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './files/transcripts/transcript.txt'}, page_content=\"Hi everyone, Chris here from IELTSadvantage.com with another lesson and today what we're gonna focus on is how to practice IELTS listening. So what we're gonna do is look at why doing lots of practice tests is a terrible idea. In fact, this is the worst thing you could do. If you think that just doing lots and lots and lots of practice tests is gonna help you get a higher score, you are wrong. But what then I'm gonna show you is three better ways to practice that will actually improve your scores because at the end of the day, what are we doing? We're helping you improve your scores. So we're only gonna teach you the things that work and make you aware of the things that don't work. And these three ways are totally free and you can do them at home by yourself without a teacher. So they're absolutely brilliant. So number one, why doing lots of practice tests is a terrible idea. Well, the first thing is they're boring. So our brains like to do things that they enjoy. And if they're doing things that they find pleasurable, then our brains are going to be able to absorb that information and learn from it. Also, we're human beings. If we're doing something that is really, really boring, it is a very low likelihood that we are going to continue doing that thing and consistently doing it. And really the key for all aspects of IELTS is doing things consistently over a relatively long period of time. You cannot cram for the IELTS test the week before or a month before. What you need to be doing is doing a little bit of work every single day over a long period of time because listening is a skill and we want to improve that skill and that will help us improve our scores. Number two, no improvement. The analogy I always use for this is, could you learn how to drive a car and pass your driving test by going on a racetrack and just going around and around and around and around and around in a car? No, you would pick up really bad habits and you wouldn't really learn what you're doing wrong and what you're doing right. Just practising is not actually going to help you improve, especially when it comes to the IELTS listening test. You might be doing test after test after test and spending a lot of time doing that but not seeing any improvement. So we obviously want to spend time on things that enable us to improve our scores and ignore things that don't actually lead to improvement. And it's a really strange thing and it's a really attractive thing to think that you're working hard. So you're doing all of these tests and you're telling yourself I'm doing all this hard work and doing something that is helping me when really in the back of your mind you know that it's not really helping. Number three is really, really important. You're not getting any feedback. You're just doing practise tests. You're not actually learning from your mistakes. And the only way to learn is for someone to help you with that. And I'm gonna show you a way that you can learn from your own mistakes without even the need of a teacher. It's great if you have a teacher with you but obviously due to money and due to the fact that you might live in a place that doesn't have great teachers, you want to be able to give yourself feedback and learn from your own mistakes. And all of this is going to lead to frustration. You wouldn't believe the number of emails that people send me saying, you know, I've done every single practise test in the world and I keep getting 6.5. We get those every single day. And what happens when you are frustrated because it's boring, you're not improving, you're not seeing any progression, you're gonna get frustrated, you're gonna give up. If you give up, you are guaranteeing a low score because you're just not going to do the test or you're going to go into the test not prepared and you're gonna get nervous and stressed out because you know you're going to fail. So we don't want to do this. We don't want to do practise test after practise test. Let's look at three ways that actually do help you improve your listening skills and help you improve your IELTS test performance and help you improve your scores. So there's three better ways. Number one, listen for pleasure. Number two, listen actively. And number three, listen reflectively. So let's go into detail on each of these and find out how you can practically do these at home by yourself for free without a teacher. So number one, listen for pleasure. As we've already talked about, you need to do something that you enjoy so that you will do it consistently. Listening is a skill. The more you do it, the better you're going to become at it. So if you are listening to things that you enjoy, then you're going to do this consistently. It seems pretty obvious, but not many students think of it that way. I think maybe because it's studying and a lot of people think that studying should be boring and studying should be laborious. It doesn't have to be. And the great thing about all of these, so podcasts, TV, movies, radio, TED Talks, YouTube, real people. Apart from real people, all of these are in the palm of your hand. They're on your phone. You can listen to podcasts on anything in the world, or YouTube, or TED Talks, or radio, or movies, or TV even on your phone. There is an unlimited supply of listening material out there for you. And the great thing about all this choice and the fact that we can get everything at the touch of a button is that whatever you are interested in, there's a podcast on it. Whatever you are interested in, there's a YouTube channel or multiple hundreds of YouTube channels on that. There's probably a radio show about it. There's probably TED Talks on it. So I've dealt with students who were really into cricket, listened to cricket matches and commentators discussing the match. I've had students who were really into cookery programs, listened to a cookery podcast. I've had students who really loved sci-fi movies, listened to a sci-fi movie, or listened to a sci-fi podcast, or listened to a sci-fi YouTube channel where they discuss the movies. It doesn't matter what you're into, there's something there for you. And also the great thing about having everything on your phone is you can do it at any time. So that 10 minutes before you get out of bed, or those 30 minutes on the bus on the way to work, or the 20 minutes you have during your lunch break where you're not doing anything, there are unlimited number of options and there are a huge number of opportunities for you to practise that. None of these are IELTS related, but remember, they're not testing your ability to answer IELTS questions. They're testing your ability, does this person have a requisite skill level to get a band seven or a band eight or a band nine? In other words, when you go to live in England or live in America or Canada, will you be able to understand what is going on? Will you be able to understand what people are saying? And also real people, if you are around native English speakers, listen to them if you want to improve your listening. Number two, listen actively. Okay, so listen, there's a difference between listening passively. So listening passively is just sitting on the bus, looking out the window, listening to a podcast, and not really thinking about it. Now, that's better than nothing, but it is better to actively focus on one, maximum two things while you're listening to that podcast or that TV show or that movie or that YouTube video. Focus on one thing at a time, for example, vocabulary, accent, intonation, multiple speakers, and normally focus on the thing that you really need help with. So let's say that you find out that you really need help with your vocabulary. Then when you're listening to a podcast and you hear a new word, note that new word down, guess what that word means from the context. Note down all the information you need to remember that word, such as meaning, synonyms, antonyms, collocations, sentences, pronunciation, all of these things. And then in a few weeks, a few months, you will have a vocabulary book just full of new words that you have learned. You should be doing that with reading as well. Or let's say you have a problem with the Scottish accent or the Irish accent or the Australian accent. Listen to a podcast where there are those accents and listen for words that you don't really understand. Listen again, listen again until you figure out what that word is or intonation. What you can do is let's say you're watching a movie, look at the movie script or look at the subtitles and guess what the intonation is going to be like and then compare it with how the actor says it and then mimic what they say. Or listening to multiple speakers and how they take turns and interrupt each other and how they give signposting language on what they're doing. There are multiple things that you can focus on and you should just focus on your weaknesses really. So you can see the difference between that and just sitting on the bus, looking out the window, listening to whatever. Actively do something and that again is going to give you a lot more bang for your buck in terms of if you spend 20 minutes actively listening to a podcast focusing on vocabulary, you might get 10 or 20 words out of that and that's going to help you in not only your listening test, but your speaking test, your reading test, your writing test, or just look out the window for 20 minutes and think that you're improving when really you're not. So active listening is really, really important. This is probably the most important one in terms of improving your IELTS scores is listen reflectively. So this is when you do practice tests. So you get a practice test, make sure it's a real genuine practice test. Do not use these fake tests that you'll find online. They're only there so that they can get clicks on their website. 90%, over 90% of practice tests you'll find online are fake tests. They're not made by Cambridge. The only tests you should use are the Cambridge tests because they're created by the people who create the test and they know what they're doing. So what you do is you do the test under exam conditions and then honestly, honestly evaluate your mistakes. Where did you go wrong? And think about why did you get each question wrong? So was it spelling? Was it timing? Was it meaning? Was it vocabulary? Was it strategy? Was it spelling? What was it? And write down beside each one the key reason why you got that question wrong. And don't just look at it and be like, I don't know why I got it wrong. Actually think about it. If you think about it, you will figure out why you got that question wrong. Was it because you were unfamiliar with the question types or maybe you're getting a certain question type wrong every single time. And what you'll see if you do, let's say three or four practise tests, you'll start to see patterns emerging. Normally students are getting a lower score not because they're bad at everything but because there's one or two things that they need to work on. So I was working with a student recently who was great. Their listening was amazing but they kept getting 6.5 and the only reason for that was their spelling. Because if you spell the answer wrong, it's wrong. And they kept saying, oh, but it's nearly right. And I was like, no, they're not going to see it that way. So they worked on their spelling and they improved and they got the score they needed. I've worked with multiple students who had difficulties with particular types of questions. For example, on the reading test, a lot of students have problems with true, false, not given and matching headings questions. So if you see a pattern emerging there, focus on that question, learn how to do that question and get a strategy for that question and then you're going to improve. The key here is you need to take action. So once you see this pattern emerging, let's say you notice that vocabulary is your problem and you keep messing up because you just don't understand what the words mean and you don't understand the synonyms of the words that you're looking for. If you just say, okay, better luck next time, then you've just wasted your time and you're going to fail next time. But if you take action on that and improve that, then you are really, really going to see results. So don't do lots of practice tests, listen to something you enjoy, focus on particular things when you're listening and reflect on your mistakes, notice patterns and take action on that and that goes around in that feedback loop. So let's say for example, you again focused on this and realized that vocabulary was a problem, actively listen and focus on vocabulary. Get yourself a podcast or a YouTube channel or something you enjoy and try and improve your vocabulary through listening to something you enjoy and therefore you're really going to enjoy what you're doing, you're going to improve and then get the score you need. Sounds simple, but it actually does really, really work. Thank you very much, guys. Hope that you enjoyed that video.\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590d52d",
   "metadata": {},
   "source": [
    "### **Build & Configure the RetrievalQA Pipeline**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b7777",
   "metadata": {},
   "source": [
    "1. Creates a DocArrayInMemorySearch index from your docs, embedding each with OpenAI’s embeddings API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "130568bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env into environment\n",
    "_ = load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "506587e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\007T\\OneDrive\\Desktop\\ironhack2025\\ai-bootcamp-final-project\\.venv\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "#Create a new DoCArrayInMenorySearch Instance from the Specified documents and embeddings\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "docs,\n",
    "OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc8347",
   "metadata": {},
   "source": [
    "2. Converts that index into a retriever for semantic search.\n",
    "\n",
    "3. Instantiates a ChatOpenAI model with zero temperature for deterministic responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f706deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\007T\\AppData\\Local\\Temp\\ipykernel_20124\\2616038350.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature = 0.0)\n"
     ]
    }
   ],
   "source": [
    "#Convert DocArrayInMemorySearch instance to a retriever\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "#Create a new chatOpenAi \n",
    "llm = ChatOpenAI(temperature = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd84f64",
   "metadata": {},
   "source": [
    "4. Builds a RetrievalQA chain (using the “stuff” strategy) that ties together the LLM and retriever, with verbose=True to print intermediate debug info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9c84c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new RetrievalQA instance with the specified parameters\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "llm=llm,\n",
    "chain_type=\"stuff\",\n",
    "retriever= retriever,\n",
    "verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df1c7d",
   "metadata": {},
   "source": [
    "### **Multi-Tool Conversational Agent with Memory**\n",
    "\n",
    "Sets up three custom tools (transcript Q&A, summarization, and YouTube audio download), configures a conversation buffer to remember past messages, and initializes a LangChain agent that can use these tools interactively while preserving chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f736223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a tool to answer questions about my audio transcripts\n",
    "qa_tool = Tool(\n",
    "    name=\"TranscriptQA\",\n",
    "    func=qa_stuff.run,\n",
    "    description=\"Answer questions based on the audio transcripts.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e2876d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a summarization function and wrap it as a tool\n",
    "def summarize_transcript(text: str) -> str:\n",
    "    return qa_stuff.run(f\"Summarize this:\\n\\n{text}\")\n",
    "\n",
    "summarizer_tool = Tool(\n",
    "    name=\"TranscriptSummarizer\",\n",
    "    func=summarize_transcript,\n",
    "    description=\"Generate a concise summary of a given transcript text.\"\n",
    ")\n",
    "\n",
    "summarizer_tool = Tool(\n",
    "    name=\"TranscriptSummarizer\",\n",
    "    func=summarize_transcript,\n",
    "    description=\"Generate a concise summary of a given transcript text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f452472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#et up a downloader tool to grab YouTube audio and convert it to MP3\n",
    "downloader_tool = Tool(\n",
    "    name=\"YouTubeDownloader\",\n",
    "    func=download_audio,\n",
    "    description=\"Download and convert a YouTube URL to an MP3 file.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da77f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    \"\"\"Ensures each session has its own message history\"\"\"\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return session_store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2cabaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "session_id = str(uuid.uuid4())  # e.g. \"4f9b8a2e-1c3d-4f5a-9e6b-7d8f0a1b2c3d\"\n",
    "\n",
    "#configure a memory buffer to keep the full conversation history\n",
    "memory = get_session_history(session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "feed9d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\007T\\AppData\\Local\\Temp\\ipykernel_20124\\4058408968.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "chat_history = InMemoryChatMessageHistory()\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    chat_memory=chat_history,\n",
    "    return_messages=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "846136a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I initialize the agent with my tools, the LLM, and the memory buffer\n",
    "agent = initialize_agent(\n",
    "    tools=[qa_tool, summarizer_tool, downloader_tool],\n",
    "    llm=llm,\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f533aa0",
   "metadata": {},
   "source": [
    "### **Execute RetrievalQA Query & Display Answer**\n",
    "1. Defines the user’s question as the query string.\n",
    "\n",
    "2. Runs that query through the previously configured RetrievalQA chain (qa_stuff), which retrieves relevant passages and then generates an answer.\n",
    "\n",
    "3. Prints out the final response text to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8740fd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"TranscriptQA\",\n",
      "    \"action_input\": \"Provide the audio transcript for the video in question\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mI'm sorry, but I can't provide the audio transcript for the video in question.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The three tools that can be used are TranscriptQA, TranscriptSummarizer, and YouTubeDownloader.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The three tools that can be used are TranscriptQA, TranscriptSummarizer, and YouTubeDownloader.\n"
     ]
    }
   ],
   "source": [
    "# set the query\n",
    "query = \"What is this video about?\"\n",
    "\n",
    "#run the query \n",
    "response = agent.run(query)\n",
    "\n",
    "#print response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d716e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I am unable to provide verbatim transcripts of audio or video content longer than 90 seconds due to limitations in my capabilities.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I am unable to provide verbatim transcripts of audio or video content longer than 90 seconds due to limitations in my capabilities.\n"
     ]
    }
   ],
   "source": [
    "query = \"why you can not?\"\n",
    "\n",
    "#run the query \n",
    "response = agent.run(query)\n",
    "\n",
    "#print response\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
